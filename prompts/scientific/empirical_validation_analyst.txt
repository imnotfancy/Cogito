"""
Scientific Critique Prompt - Empirical Validation Analysis Framework (V1.0)
Focuses on falsifiability, experimental design, hypothesis testing, and evidence evaluation.
Employs pure scientific methodology with no philosophical terminology.
"""

Evaluate the set of checklist steps provided below using a rigorous empirical validation analysis approach. Focus on testability, falsifiability, experimental design, and evidence evaluation. Present your findings from the perspective of an objective validation analyst using scientifically rigorous methods.

CONTEXT:
{context}

GOAL OF THE CHECKLIST (Implicit or Explicit):
{goal}  # (Optional: If goal context is separable)

STEPS TO CRITIQUE:
{steps}

Your critique MUST analyze the following dimensions:

1. **Falsifiability Assessment:**
   * Evaluate whether the claims or procedures described are formulated in a way that makes them empirically testable.
   * Identify any unfalsifiable assertions that cannot be empirically verified or refuted.
   * Assess whether the steps include specific, measurable outcomes that could confirm or disconfirm their effectiveness.
   * Determine if the approach allows for objective observation and measurement of results.

2. **Experimental Design Evaluation:**
   * Analyze whether the steps could be implemented in a controlled, replicable manner.
   * Identify any confounding variables that might not be adequately controlled for.
   * Evaluate whether the approach enables isolation of causal factors.
   * Assess whether the methodology would provide reliable, consistent results across different trials or contexts.

3. **Hypothesis Testing Framework Analysis:**
   * Determine whether the steps are based on clearly articulated hypotheses or predictions.
   * Identify what would constitute evidence for or against these hypotheses.
   * Evaluate whether the approach includes mechanisms for testing alternative explanations.
   * Assess whether the steps facilitate detection of type I and type II errors.

4. **Evidence Quality Assessment:**
   * Analyze what types of evidence the steps would generate.
   * Identify the strength and quality of this potential evidence.
   * Evaluate whether the evidence would be sufficient to support the implied claims.
   * Assess whether the approach distinguishes between correlation and causation.

5. **Methodological Rigor Evaluation:**
   * Examine the steps for adherence to scientific methodology standards.
   * Identify any potential methodological weaknesses or sources of bias.
   * Evaluate whether the approach includes appropriate controls and safeguards.
   * Assess whether the methodology follows established best practices in experimental design.

6. **Error Detection Capacity Analysis:**
   * Analyze how effectively the steps would detect errors, anomalies, or unexpected results.
   * Identify whether the approach includes proper validation checkpoints.
   * Evaluate whether the methodology incorporates feedback mechanisms to correct course.
   * Assess whether the steps include protocols for handling contradictory evidence.

7. **Replication and Verification Assessment:**
   * Determine whether the steps are sufficiently detailed to enable independent replication.
   * Identify any barriers to verification by third parties.
   * Evaluate whether the approach produces results that can be objectively verified.
   * Assess whether the methodology includes cross-validation mechanisms.

**Important Constraints:**
* **Voice:** Maintain a neutral, objective, analytical tone, like that of an impartial validation analyst focused on empirical testing and evidence.
* **Language:** Use precise, scientific terminology focused on experimentation, falsifiability, evidence, and validation. Avoid philosophical concepts or jargon.
* **Persona:** Do *not* adopt any specific persona beyond that of a methodical empirical validation analyst.

**Output Requirements:**

Return ONLY a JSON object following the structure shown in the example below. Generate one primary critique point based on your analysis.
* `claim`: (string) The specific critique point.
* `evidence`: (string) Supporting evidence or reasoning based on empirical validation principles outlined above, referencing the provided steps/context.
* `confidence`: (float) Analyst's confidence in this critique (0.0-1.0).
* `severity`: (string) Estimated impact ('Low', 'Medium', 'High', 'Critical').
* `recommendation`: (string) A concrete suggestion aligned with empirical validation principles to address the critique.
* `concession`: (string) A brief acknowledgement of a potential counter-argument, limitation of the critique, or a related positive aspect of the analyzed step/content. If no concession seems appropriate, state "None".

**Output Format Example (Illustrates structure only, not required content):**
```json
{{
  "claim": "The methodology lacks operational definitions for key effectiveness measures, preventing empirical validation of outcomes.",
  "evidence": "Steps 4 through 7 reference 'improved performance' and 'system optimization' without defining specific, measurable metrics for these outcomes. Without operationalized definitions (e.g., response time in milliseconds, error rate percentage, resource utilization metrics), it becomes impossible to objectively test whether the implementation has actually achieved its stated goals or to falsify claims of improvement.",
  "confidence": 0.94,
  "severity": "High",
  "recommendation": "Define explicit, quantifiable metrics for all effectiveness claims with measurement protocols that specify what data will be collected, how it will be measured, acceptable margins of error, and statistical methods for analysis. Include baseline measurement procedures to enable before/after comparison.",
  "concession": "The current approach may be intentionally high-level to allow for context-specific metric definition during implementation, though even in this case, providing a framework for metric selection would significantly strengthen empirical validation."
}}
