"""
Critique Prompt for Checklist Steps - Critical Rationalist Framework (V2.3)
Focuses on underlying reasoning patterns (problem-solving, testability, error detection), avoids persona/jargon, requires all output parameters. Ready for code integration.
"""

Evaluate the set of checklist steps provided below using a critical rationalist approach. Focus on how well these steps function as a proposed solution to a defined problem, their openness to criticism and testing, and their mechanisms for identifying and eliminating errors. Avoid assessing the steps based on justification or verification of their truth. Present your findings from the perspective of an objective, unnamed analyst emphasizing critical evaluation, problem-solving, and skepticism towards claims of certainty.

CONTEXT:
{context}

GOAL OF THE CHECKLIST (Problem Definition):
{goal}  # (Frame the goal as the initial problem to be solved)

STEPS TO CRITIQUE (Proposed Solution/Hypothesis):
{steps}

Your critique MUST rigorously apply the following principles of critical analysis:

1.  **Problem Definition:**
    * Is the specific problem the checklist aims to solve clearly articulated? Is the problem well-defined and significant within the given context?
    * Do the steps directly address this defined problem, or do they risk addressing symptoms, secondary issues, or deviating significantly?

2.  **Steps as Proposed Solutions (Hypotheses):**
    * Treat the sequence of steps as a *proposed, tentative solution* or *hypothesis* for solving the defined problem. The focus is on its adequacy as a proposal to be tested, not its justification.
    * Are the steps presented clearly and boldly, allowing for straightforward testing and criticism, or are they vague, ambiguous, or hedged in ways that make evaluation difficult?

3.  **Testability and Potential for Refutation:**
    * Identify the core claims about effectiveness or correctness implicit in the steps. Are these claims *testable*? Specifically, can one design or conceive of a practical test, observation, logical argument, or scenario that could potentially *refute* or show the inadequacy of these steps or their underlying assumptions?
    * Describe *how* these steps could be critically tested. What specific outcomes or observations would constitute a failure or refutation of this proposed solution?
    * Challenge any steps or claims that are inherently untestable (e.g., tautological, overly vague, defined in a way that precludes empirical or logical challenge).

4.  **Mechanisms for Error Detection and Correction:**
    * Assess the mechanisms *within* the steps designed for detecting errors during execution. Are there explicit checks, validation points, tests, feedback loops, or comparison points intended to identify when something has gone wrong or deviated from the expected path?
    * How are detected errors addressed? Do the steps facilitate learning from mistakes and correcting the process or the solution itself (error correction)?
    * Evaluate the *rigor* of the embedded tests or checks. Do they represent genuine attempts to find potential flaws, or are they superficial?

5.  **Potential for New Problems:**
    * Consider the likely outcomes and consequences of executing these steps. What new problems, risks, or unintended negative consequences might arise from implementing this proposed solution, even if it successfully addresses the initial problem?

6.  **Clarity and Simplicity for Testability:**
    * Are the steps presented with sufficient clarity and simplicity to facilitate understanding and critical testing? Unnecessary complexity or ambiguity can obscure potential flaws and hinder effective evaluation. (Focus on simplicity that aids testability, not necessarily ease of execution).

7.  **Critique of Justification and Induction:**
    * Critically examine if any steps, or the overall approach, implicitly rely on justifying the solution as definitively true/correct/probable or on inductive reasoning (assuming future success based solely on past instances). The focus should be on the solution's ability to withstand criticism and testing, not on proving it right beforehand.

8.  **Potential for Progress (Problem-Solving Capacity):**
    * While absolute correctness is not the focus, assess whether implementing and critically testing these steps is likely to lead towards a *better* solution—one that more effectively addresses the problem or has greater problem-solving capacity—compared to plausible alternatives or doing nothing. Does the process facilitate learning and improvement?

**Important Constraints:**
* **Voice:** Maintain a critical, rational, pragmatic tone focused on problem-solving through error detection and elimination, skeptical of claims to justification or certainty, like that of an unnamed, impartial analyst.
* **Language:** Avoid using specialized philosophical jargon (e.g., 'falsificationism', 'verisimilitude', 'P1->TT->EE->P2'). Frame the analysis using generally understandable terms related to problem-solving, testing, error detection, critical evaluation, and hypothesis.
* **Persona:** Do *not* adopt a persona or refer to any specific philosopher.

**Output Requirements:**

Return ONLY a JSON object following the structure shown in the example below. Generate one primary critique point based on your analysis. **All keys listed below (`claim`, `evidence`, `confidence`, `severity`, `recommendation`, `concession`) are required in the output JSON object.**
* `claim`: (string) The specific critique point regarding lack of testability, poor error detection, reliance on justification/induction, potential for new problems, etc.
* `evidence`: (string) Explanation referencing specific steps and the principles of critical evaluation, testability, or error detection being violated or lacking.
* `confidence`: (float) Agent's confidence in this critique (0.0-1.0).
* `severity`: (string) Estimated impact ('Low', 'Medium', 'High', 'Critical').
* `recommendation`: (string) A concrete suggestion to increase testability, improve error detection, clarify the problem/steps, or frame the steps more effectively as a testable hypothesis.
* `concession`: (string) A brief acknowledgement of practical constraints or context that might explain why a step is formulated in a less-than-ideal way regarding testability (e.g., "While direct testing of this assumption is difficult, it may be a necessary prerequisite based on external system limitations," or "The complexity of the domain makes designing simple, rigorous tests challenging within the scope of this checklist."). If no concession seems appropriate, state "None".

**Output Format Example (Illustrates structure only, not required content):**
```json
{{
  "claim": "Step 5's success criterion ('ensure optimal performance') is too vague to be effectively tested or refuted.",
  "evidence": "The term 'optimal performance' is not defined with measurable parameters, making it impossible to design a clear test that could demonstrate failure to meet the criterion. Any outcome could potentially be argued as 'optimal' under some interpretation.",
  "confidence": 0.9,
  "severity": "High",
  "recommendation": "Replace 'ensure optimal performance' with specific, measurable, and testable criteria (e.g., 'achieve response time below 100ms for query X', 'maintain CPU usage below 70% under load Y').",
  "concession": "However, defining precise 'optimal' metrics might require extensive preliminary analysis outside the scope of this specific checklist, making a vaguer term a pragmatic placeholder."
}}